<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-12-02">

<title>how camera resolution and distance affects facial recognition accuracy (simplified analysis) – riki’s site</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">riki’s site</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../02_articles.html"> 
<span class="menu-text">articles &amp; papers</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../01_projects.html"> 
<span class="menu-text">projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../essay/index_essay.html"> 
<span class="menu-text">essays</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../cdb/index_cdb.html"> 
<span class="menu-text">ChatGPT’s daily brief</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#tldr" id="toc-tldr" class="nav-link active" data-scroll-target="#tldr">tldr!</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">1. introduction</a></li>
  <li><a href="#why-distance-and-resolution-matter" id="toc-why-distance-and-resolution-matter" class="nav-link" data-scroll-target="#why-distance-and-resolution-matter">2. why distance and resolution matter</a>
  <ul class="collapse">
  <li><a href="#face-resolution-thresholds" id="toc-face-resolution-thresholds" class="nav-link" data-scroll-target="#face-resolution-thresholds">face resolution thresholds</a></li>
  </ul></li>
  <li><a href="#the-mathematical-relationship" id="toc-the-mathematical-relationship" class="nav-link" data-scroll-target="#the-mathematical-relationship">3. the mathematical relationship</a>
  <ul class="collapse">
  <li><a href="#derivation" id="toc-derivation" class="nav-link" data-scroll-target="#derivation">Derivation</a></li>
  <li><a href="#final-relationship" id="toc-final-relationship" class="nav-link" data-scroll-target="#final-relationship">Final Relationship</a></li>
  <li><a href="#interpretation" id="toc-interpretation" class="nav-link" data-scroll-target="#interpretation">Interpretation</a></li>
  <li><a href="#example-4k-camera-at-different-distances" id="toc-example-4k-camera-at-different-distances" class="nav-link" data-scroll-target="#example-4k-camera-at-different-distances">example: 4K camera at different distances</a></li>
  </ul></li>
  <li><a href="#recognition-risk-zones-interactive-risk-heatmap" id="toc-recognition-risk-zones-interactive-risk-heatmap" class="nav-link" data-scroll-target="#recognition-risk-zones-interactive-risk-heatmap">4. recognition risk zones (interactive risk heatmap)</a></li>
  <li><a href="#practical-lessons-and-takeaways" id="toc-practical-lessons-and-takeaways" class="nav-link" data-scroll-target="#practical-lessons-and-takeaways">5. practical lessons and takeaways</a></li>
  <li><a href="#concluding-notes" id="toc-concluding-notes" class="nav-link" data-scroll-target="#concluding-notes">6. concluding notes</a></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix">7. appendix</a>
  <ul class="collapse">
  <li><a href="#methods-assumptions-for-facial-pixel-thresholds" id="toc-methods-assumptions-for-facial-pixel-thresholds" class="nav-link" data-scroll-target="#methods-assumptions-for-facial-pixel-thresholds">methods &amp; assumptions for facial pixel thresholds</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">how camera resolution and distance affects facial recognition accuracy (simplified analysis)</h1>
<p class="subtitle lead">a practical, quantitative guide for privacy curious folks</p>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 2, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">tldr!</h2>
<p>Facial-recognition models such as <a href="https://www.insightface.ai/research/arcface">ArcFace</a>, <a href="https://en.wikipedia.org/wiki/FaceNet">FaceNet</a>, and modern <a href="https://www.insightface.ai">InsightFace</a> pipelines rely on <strong>image detail</strong> to extract reliable facial “embeddings” (i.e., mathematical representations of the image of a face). One of the strongest predictors of recognition accuracy comes from a much more fundamental factor: <strong>the number of pixels covering the face</strong>. This post explains the mathematical relationship between <strong>camera resolution</strong>, <strong>distance</strong>, and <strong>facial-recognition accuracy</strong>, and provides a practical Python tool to measure facial image quality and associated privacy risk.</p>
<hr>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">1. introduction</h2>
<p>Over the past few years, consumer/government smart-camera ecosystems have come under increased scrutiny of violating personal privacy. In <a href="https://web.archive.org/web/20251001082647/https://www.theverge.com/news/788290/ring-amazon-hardware-event-price-specs">September 2025, Amazon announced a range of new 2K and 4K smart-camera devices with new AI facial recognition features</a> that stores and processes the <strong>facial embeddings of anyone passing in front of a Ring camera</strong>. Then, in October 2025, Amazon announced <a href="https://web.archive.org/web/20251201224341/https://www.cnbc.com/2025/10/16/amazon-ring-cameras-surveillance-law-enforcement-crime-police-investigations.html">Ring would partner with Flock Safety and share data with the government.</a> To be clear, <strong>Amazon is neither the first nor the only company</strong> that has released AI facial recognition features - Clearview AI, with a more law-enformence focused user-base, have also come under scrutiny.</p>
<p>Regardless, the fact remains that some individuals may worry that their <strong>face may be retained without their knowledge or consent</strong>.</p>
<p>The proliferation of facial recognition raises an important question that many reports don’t seem to adequately answer:</p>
<blockquote class="blockquote">
<p>Under <em>what conditions</em> can a doorbell or security camera actually <em>capture enough detail</em> to generate a usable facial embedding?</p>
</blockquote>
<p>Moreover, a key technical point often missing from the privacy debate:</p>
<blockquote class="blockquote">
<p>Whether a given camera <em>can</em> generate a meaningful facial embedding depends largely on <strong>distance</strong>, <strong>resolution</strong>, and the <strong>number of pixels covering the face</strong>.</p>
</blockquote>
<p>If a person walks several meters away from a camera that records only 1080px video, their face may occupy too few pixels for any modern recognition algorithm (even a sophisticated one like ArcFace or the systems deployed in commercial products) to reliably extract a face embedding at all. Conversely, a <strong>close-range camera with 4K resolution</strong> can capture enough detail to produce an embedding even from quick or casual exposures.</p>
<p>In an excellent essay from 2021, Adam Harvey in “What is a face?” <a href="https://adam.harvey.studio/what-is-a-face/">notes</a>:</p>
<blockquote class="blockquote">
<p>“The best accuracy is obtained from faces appearing in turnstile video clips with mean minimum and maximum interocular distances of 20 and 55 pixels respectively. More recently, Clearview, a company that provides face recognition technologies to law enforcement agencies, claimed to use a 110 × 110 pixel definition of a face… The face is not a single biometric but includes many sub-biometrics, each with varying levels of identification possibilities.”</p>
</blockquote>
<p>Understanding these variables is essential for evaluating <strong>realistic privacy risks</strong>:</p>
<ul>
<li>At very close range (1–2 meters), consumer cameras can produce <strong>high-quality embeddings</strong>.<br>
</li>
<li>At moderate distances, recognition quality depends heavily on resolution, angle, machine learning model, lighting, and sensor size.</li>
<li>Beyond certain pixel thresholds, a camera simply <strong>cannot</strong> extract a usable embedding, no matter what software is used. Gen AI “image enhancers” will try to claim differently, but realistically no. They impute (creating uncertainty), and do not “add” more information.</li>
</ul>
<p>The goal in this essay is to use a straightforward geometric framework that helps you estimate the <strong>facial pixels</strong> captured by cameras. Using just four ingredients^ - 1) camera resolution, 2) field of view, 3) real-world face size, and 4) distance - you can loosely estimate whether a facial recognition system is likely to succeed or fail. Understanding these limits enables us to think more precisely about the <strong>technical privacy implications</strong> of today’s ubiquitous home surveillance devices.</p>
<p>^ Disclaimer! To clarify, there are more than four ingredients. For example, blur caused by movement would understandably impact facial recognition. Blur, of course, partly results from a video camera’s frame rate. Likewise, low light conditions would also affect the quality of the facial embedding. To keep things simple, I’m using a “naive” framework :P</p>
</section>
<section id="why-distance-and-resolution-matter" class="level2">
<h2 class="anchored" data-anchor-id="why-distance-and-resolution-matter">2. why distance and resolution matter</h2>
<p>Facial-recognition systems do not “see” distance directly. What they <em>actually</em> see is the <strong>face as a block of pixels</strong>. As a person moves farther away from the camera, their face occupies fewer pixels, reducing the amount of detail available for landmark extraction and embedding generation.</p>
<blockquote class="blockquote">
<p><strong>More pixels per face → more datapoints per face → better recognition. Fewer pixels → degraded or impossible recognition.</strong></p>
</blockquote>
<p>This is why a person standing directly in front of a camera is recognized with high confidence, while someone several meters away may not even be detected.</p>
<p>Here’s a <a href="http://digitalprotalk.blogspot.com/2008/02/so-how-many-pixels-does-it-take-to-make.html#:~:text=As%20cameras%20continue%20to%20improve,the%20more%20pixels%20%2C%20the%20better.">blog post</a> from photographer David Ziser about pixels and portraits</p>
<p><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVvjiC8z3t-J0HUd2WPwNyXlKmqw5Azqr-G_8nN09fm0fk9RtjgZ2EH8OLaY8qpRnEp1CqIyiMkbEbcQs2JbZi4JejclSiZ-iCo7pquaWzvJSdoe79ls88l18JfbJKW1a4iE66/s400/FAcePix+4-Image+Compare.jpg" width="300" style="border-radius: 10px; float: center;" alt="pixel and portraits"></p>
<section id="face-resolution-thresholds" class="level3">
<h3 class="anchored" data-anchor-id="face-resolution-thresholds">face resolution thresholds</h3>
<p>Research (see appendix for further info on methods!) on face-recognition systems shows:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Face Pixel Height</th>
<th>Expected Recognition Quality</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>≥ 200 px</strong></td>
<td>Excellent (high stability, robust to disguises)</td>
</tr>
<tr class="even">
<td><strong>112 px</strong></td>
<td>Standard threshold for ArcFace</td>
</tr>
<tr class="odd">
<td><strong>80 px</strong></td>
<td>Noticeable performance drop</td>
</tr>
<tr class="even">
<td><strong>&lt; 40 px</strong></td>
<td>Recognition typically fails</td>
</tr>
</tbody>
</table>
<p>Now comes some maths - if you’d rather skip it, go straight to part 4!</p>
<hr>
</section>
</section>
<section id="the-mathematical-relationship" class="level2">
<h2 class="anchored" data-anchor-id="the-mathematical-relationship">3. the mathematical relationship</h2>
<p>We can relate <strong>camera resolution</strong>, <strong>field of view</strong>, and <strong>distance</strong> to the <strong>face height in pixels</strong> using a simple geometric projection formula.</p>
<p>Let:</p>
<ul>
<li><span class="math inline">\((H_f)\)</span> = real face height (meters), e.g.&nbsp;<span class="math inline">\(( H_f \approx 0.20\,\text{m} )\)</span></li>
<li><span class="math inline">\(( R_v )\)</span> = vertical resolution of the camera (pixels), e.g.&nbsp;720, 1080, 2160</li>
<li><span class="math inline">\(( \theta )\)</span> = vertical field of view (in degrees)</li>
<li><span class="math inline">\(( D )\)</span> = distance from the camera to the subject (meters)</li>
<li><span class="math inline">\(( \text{face}_{px} )\)</span> = face height in pixels in the captured image</li>
</ul>
<section id="derivation" class="level3">
<h3 class="anchored" data-anchor-id="derivation">Derivation</h3>
<p>The vertical span of the scene at distance <span class="math inline">\(D\)</span> is:</p>
<p><span class="math display">\[
H_{\text{scene}} = 2 D \tan\left(\frac{\theta}{2}\right).
\]</span></p>
<p>This full vertical span is mapped to <span class="math inline">\(R_v\)</span> pixels, so the <strong>meters-per-pixel</strong> in the vertical direction is:</p>
<p><span class="math display">\[
\text{meters-per-pixel} = \frac{H_{\text{scene}}}{R_v}
= \frac{2 D \tan\left(\frac{\theta}{2}\right)}{R_v}.
\]</span></p>
<p>The face height in pixels is then:</p>
<p><span class="math display">\[
\text{face}_{px}
= \frac{H_f}{\text{meters-per-pixel}}
= \frac{H_f}{\dfrac{2 D \tan\left(\frac{\theta}{2}\right)}{R_v}}
= \frac{R_v H_f}{2 D \tan\left(\frac{\theta}{2}\right)}.
\]</span></p>
</section>
<section id="final-relationship" class="level3">
<h3 class="anchored" data-anchor-id="final-relationship">Final Relationship</h3>
<p><span class="math display">\[
\boxed{
\text{face}_{px}
= \frac{R_v \, H_f}{2 D \, \tan\left(\dfrac{\theta}{2}\right)}
}
\]</span></p>
<p>This shows:</p>
<ul>
<li><span class="math inline">\(\text{face}_{px} \propto R_v\)</span>: higher resolution → more pixels on the face<br>
</li>
<li><span class="math inline">\(\text{face}_{px} \propto \frac{1}{D}\)</span>: farther away → fewer pixels on the face<br>
</li>
<li><span class="math inline">\(\text{face}_{px} \propto \frac{1}{\tan(\theta/2)}\)</span>: narrower FOV (zoomed-in) → more pixels on the face</li>
</ul>
</section>
<section id="interpretation" class="level3">
<h3 class="anchored" data-anchor-id="interpretation">Interpretation</h3>
<ul>
<li><strong>Double the distance → half the face pixels</strong><br>
</li>
<li><strong>Double the resolution → double the face pixels</strong><br>
</li>
<li><strong>Narrower field of view → higher pixel density on the face</strong></li>
</ul>
<p>This explains why a 4K camera can reliably detect faces at much longer distances than a 1080px webcam.</p>
</section>
<section id="example-4k-camera-at-different-distances" class="level3">
<h3 class="anchored" data-anchor-id="example-4k-camera-at-different-distances">example: 4K camera at different distances</h3>
<p>Assume:</p>
<ul>
<li><span class="math inline">\(H_f = 0.20\)</span> m, see <a href="https://www.quora.com/What-is-the-average-length-of-a-mans-face-I-am-21-and-my-face-length-is-7-inches">this</a>.</li>
<li><span class="math inline">\(R_v = 2160\)</span> px (4K)<br>
</li>
<li><span class="math inline">\(\theta = 90^\circ\)</span></li>
</ul>
<p>Using the formula:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Distance (meters)</th>
<th>Face Pixel Height (px)</th>
<th>Implied Recognition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>1 m</strong></td>
<td>~216 px</td>
<td>Excellent</td>
</tr>
<tr class="even">
<td><strong>2 m</strong></td>
<td>~108 px</td>
<td>Usable</td>
</tr>
<tr class="odd">
<td><strong>3 m</strong></td>
<td>~72 px</td>
<td>Weak</td>
</tr>
<tr class="even">
<td><strong>5 m</strong></td>
<td>~43 px</td>
<td>Often fails</td>
</tr>
</tbody>
</table>
<hr>
</section>
</section>
<section id="recognition-risk-zones-interactive-risk-heatmap" class="level2">
<h2 class="anchored" data-anchor-id="recognition-risk-zones-interactive-risk-heatmap">4. recognition risk zones (interactive risk heatmap)</h2>
<p>The embedded app below shows how different smart-camera devices compare in terms of privacy risk at various distances. Darker/red cells indicate conditions where a camera is more likely to capture enough pixels to generate a usable facial embedding.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Launch the Interactive Heatmap App
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="https://app-privacyriskheatmap-camerafacialembed.streamlit.app/?embed_options=light_theme">Click here to open the Streamlit app live →</a></p>
</div>
</div>
<p>Code available <a href="https://github.com/rikimatsumoto/streamlit-privacyriskheatmap">here</a></p>
<hr>
</section>
<section id="practical-lessons-and-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="practical-lessons-and-takeaways">5. practical lessons and takeaways</h2>
<section id="a.-distance-acts-as-a-form-of-compression" class="level4">
<h4 class="anchored" data-anchor-id="a.-distance-acts-as-a-form-of-compression">a. Distance acts as a form of compression!</h4>
<p>Moving farther from the camera reduces image detail <em>exactly</em> like lowering resolution. Duh…</p>
</section>
<section id="b.-camera-resolution-directly-determines-usable-recognition-range" class="level4">
<h4 class="anchored" data-anchor-id="b.-camera-resolution-directly-determines-usable-recognition-range">b. Camera resolution directly determines usable recognition range</h4>
<p>A 4K camera offers roughly <strong>2× the effective distance</strong> of a 1080px camera for the same accuracy.</p>
</section>
<section id="c.-face-pixel-count-predicts-accuracy-better-than-any-other-variable" class="level4">
<h4 class="anchored" data-anchor-id="c.-face-pixel-count-predicts-accuracy-better-than-any-other-variable">c.&nbsp;Face pixel count predicts accuracy better than any other variable</h4>
<p>This often matters more than:</p>
<ul>
<li>model architecture</li>
<li>training dataset</li>
<li>pre-processing steps</li>
</ul>
</section>
<section id="d.-optical-zoom-dramatically-improves-performance" class="level4">
<h4 class="anchored" data-anchor-id="d.-optical-zoom-dramatically-improves-performance">d.&nbsp;Optical zoom dramatically improves performance</h4>
<p>But digital zoom does not…</p>
</section>
<section id="e.-bad-lighting-cancels-out-high-resolution" class="level4">
<h4 class="anchored" data-anchor-id="e.-bad-lighting-cancels-out-high-resolution">e. Bad lighting cancels out high resolution</h4>
<p>Noise from low light conditions reduces the pixel quality. <a href="https://web.archive.org/web/20251203044605/https://www.sdmmag.com/articles/104360-image-clarity-requires-more-than-resolution-alone">Higher resolution means more detail, but increasing pixel count shrinks pixel size, reducing light intake and affecting low-light performance. But (!!!) larger sensors in newer cameras enable more pixels without sacrificing size.</a></p>
<hr>
</section>
</section>
<section id="concluding-notes" class="level2">
<h2 class="anchored" data-anchor-id="concluding-notes">6. concluding notes</h2>
<p>Facial-recognition accuracy follows a simple rule: <strong>the number of pixels covering the face.</strong> Knowing this helps privacy-conscious folks estimate when a camera can reliably identify someone, setting realistic expectations for when you should be concerned. For example, a Ring device with 4K resolution, standing more than 5 meters away (like on a house’s front lawn) is <em>probably</em> safe…</p>
<p>The distance from the house (where a camera might be placed) to the street largely depends on local zoning laws. Local codes might require houses to be a certain distance (e.g., 20-40 ft) from the street, which directly determines the minimum front yard depth. In suburban America, houses are probably located 6-12 meters (20-40 ft) from the street, so pedestrians on the street likely <em>don’t have to worry</em> about facial recognition.</p>
<hr>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">7. appendix</h2>
<section id="methods-assumptions-for-facial-pixel-thresholds" class="level3">
<h3 class="anchored" data-anchor-id="methods-assumptions-for-facial-pixel-thresholds">methods &amp; assumptions for facial pixel thresholds</h3>
<section id="a.-why-face-height-in-pixels" class="level4">
<h4 class="anchored" data-anchor-id="a.-why-face-height-in-pixels">a. why face height in pixels?</h4>
<p>In facial recognition standards, the term <strong>inter-ocular distance (IOD)</strong> - the number of pixels between the centers of the eyes - rather than full face height is used. To translate IOD, I assumed the following proportionality:</p>
<p><span class="math display">\[
\text{Face height} \approx 2.5 \times \text{IOD}.
\]</span></p>
<p>Personally, I’m probably closer to 3 than 2.5.</p>
</section>
<section id="b.-evidence-used-to-define-the-thresholds" class="level4">
<h4 class="anchored" data-anchor-id="b.-evidence-used-to-define-the-thresholds">b. Evidence used to define the thresholds</h4>
<p><em>I. High-quality threshold (≥ 200 px face height)</em> High-performance facial recognition typically requires <strong>large IOD values</strong>:</p>
<ul>
<li>ISO and ICAO guidelines for travel documents recommend <strong>90–120 px IOD</strong> for high-quality face images.</li>
<li>Commercial/NIST-linked guidance for FR systems also recommends <strong>≥ 90 px</strong>, preferably <strong>≥ 120 px</strong> IOD.</li>
</ul>
<p>Converted via the 2.5× proportion, these imply:</p>
<p><span class="math display">\[
\text{Face height} \approx 225\text{–}300\ \text{px}.
\]</span></p>
<p>To avoid overstating precision, I adopt <strong>≥ 200 px</strong> as a conservative indicator that face recognition is likely to be <strong>reliable</strong> for many modern models.</p>
<p><em>II. Usable / “medium” threshold (≈ 112 px)</em></p>
<p>Recent deep face-recognition networks like <strong>ArcFace</strong> (CVPR 2019), were trained on <strong>112×112 px aligned face crops</strong>. This suggests the model expects roughly that pixel scale for stable embeddings.</p>
<p>Thus:</p>
<p><span class="math display">\[
\text{Standard “usable” face height} \approx 112\ \text{px}.
\]</span></p>
<p><em>III. Low-quality threshold (≈ 80 px)</em></p>
<p>The choice of 80 px comes from two research lines:</p>
<ol type="1">
<li><strong>Low-resolution face recognition surveys</strong> indicate that traditional algorithms require <strong>32×32–64×64 px</strong> face crops to function, with performance dropping sharply below this range.</li>
<li>A NIST-affiliated study (Hu et al.) found that systems maintained comparable accuracy as IOD decreased from <strong>60 px to 30 px</strong>, but performance <strong>severely degraded</strong> at <strong>15 px IOD</strong>.</li>
</ol>
<p>Using the 2.5× conversion:</p>
<ul>
<li>30 px IOD → ~75 px face height.</li>
</ul>
<p>Therefore <strong>~80 px</strong> marks a reasonable lower boundary for <strong>weak but still potentially recognizable</strong> faces.</p>
<p><em>IV. Minimal threshold (&lt; 40 px)</em></p>
<p>Several studies show that below this scale, recognition becomes unreliable:</p>
<ul>
<li>NIST (Hu et al.) reports severe degradation at <strong>15 px IOD</strong> (~38 px face height).</li>
<li>Zou &amp; Yuen describe <strong>faces smaller than 16×16 px</strong> as “very-low-resolution,” where recognition algorithms perform poorly or fail entirely.</li>
<li>2020 PULSE project by Duke University researchers showed that the restricted perceptual space of a 16 × 16 pixel face image allows for wildly different identities to all downscale to perceptually indifferent images. The work confirmed the technical reality that <strong>two faces can appear identical at 16 × 16 pixels, but resemble completely different identities at 1024 × 1024 pixels</strong>.</li>
</ul>
<p>Thus, <strong>&lt; 40 px face height</strong> is used to indicate <strong>minimal recognition capability</strong>.</p>
</section>
<section id="c.-limitations-and-uncertainty" class="level4">
<h4 class="anchored" data-anchor-id="c.-limitations-and-uncertainty">c.&nbsp;Limitations and uncertainty</h4>
<p>These thresholds should be interpreted as <strong>heuristic ranges</strong>:</p>
<ul>
<li>Recognition performance depends on lighting, pose, occlusion, compression, and camera optics—not just pixel count.</li>
<li>Models trained on super-resolution or cross-resolution datasets may recognize faces below these thresholds in controlled settings.</li>
<li>Many facial recognition models assume a frontal face and simple geometric projection; real-world scenes introduce additional variability.</li>
</ul>


</section>
</section>
</section>

</main> <!-- /main -->
<hr style="margin-top:2em;margin-bottom:1em;">

<p><strong>Disclaimer:</strong> This essay/blog contains my own views and do not reflect my employer. My essays may contain errors, omissions, or outdated information. It is provided “as is,” without warranties of any kind, express or implied. The essays are not investment, legal, security, or policy advice and must not be relied upon for decision-making. You are responsible for independently verifying facts and conclusions. The author does not accept any liability for losses or harms arising from use of this content. No duty to update is assumed.</p>


<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>